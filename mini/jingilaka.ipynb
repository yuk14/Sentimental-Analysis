{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7128c8-400c-490e-82d9-4e5b3860dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\SIRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Sentiment\n",
      "count  4267.000000\n",
      "mean      0.092501\n",
      "std       0.476676\n",
      "min      -0.995700\n",
      "25%      -0.153100\n",
      "50%       0.000000\n",
      "75%       0.458800\n",
      "max       0.981400\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'csv_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Provide link to download CSV file\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m download_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<a href=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m download>Click here to download the CSV file</a>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     46\u001b[0m display(HTML(download_link))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Categorize sentiments\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv_path' is not defined"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='QtW5i86wx_goa7ojeKNWQQ',\n",
    "    client_secret='-fl2QLqMuinZfdWDTt2JrUoFD8E7zg',\n",
    "    user_agent='sentimental_analysis_project:v1.0.0 (by u/Realistic-Youth-4325)'\n",
    ")\n",
    "\n",
    "# Choose a subreddit and time period\n",
    "subreddit_name = 'movies'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "posts = subreddit.top(time_filter='week', limit=10)\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Prepare data for analysis\n",
    "data = []\n",
    "for post in posts:\n",
    "    post.comments.replace_more(limit=0)\n",
    "    for comment in post.comments.list():\n",
    "        sentiment = sid.polarity_scores(comment.body)\n",
    "        comment_time = datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        data.append([comment.body, sentiment['compound'], comment_time])\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data, columns=['Comment', 'Sentiment', 'Time'])\n",
    "\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "df.to_csv('reddit_sentiment_analysis.csv', index=False)\n",
    "\n",
    "# Display basic sentiment analysis results\n",
    "print(df.describe())\n",
    "\n",
    "# Provide link to download CSV file\n",
    "download_link = f'<a href=\"{csv_path}\" download>Click here to download the CSV file</a>'\n",
    "display(HTML(download_link))\n",
    "\n",
    "# Categorize sentiments\n",
    "def categorize_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment_Category'] = df['Sentiment'].apply(categorize_sentiment)\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['Cleaned_Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# Feature: Display visualizations\n",
    "def plot_visualizations(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Pie chart for sentiment distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    df['Sentiment_Category'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    # Bar chart for sentiment distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.countplot(x='Sentiment_Category', data=df, palette='viridis')\n",
    "    plt.title('Sentiment Count')\n",
    "    \n",
    "    # Word count distribution\n",
    "    df['Word_Count'] = df['Comment'].apply(lambda x: len(x.split()))\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(df['Word_Count'], bins=30, kde=True)\n",
    "    plt.title('Word Count Distribution')\n",
    "    \n",
    "    # Scatter plot for sentiment scores\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.scatterplot(x='Word_Count', y='Sentiment', hue='Sentiment_Category', data=df, palette='viridis')\n",
    "    plt.title('Sentiment Scores vs Word Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_visualizations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32cf681-3cad-4aa0-a4fc-931a90d0c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb2e037-58b9-4585-afbb-eeb6bb7314c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[43mmodel\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump(model, 'sub.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e399be-3ba5-44ac-99a3-91e61a2cb0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
